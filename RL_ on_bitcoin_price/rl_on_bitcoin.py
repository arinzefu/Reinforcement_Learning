# -*- coding: utf-8 -*-
"""RL_on_bitcoin.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TBG8FTqCWUwiC5EsWvW0bZGUIV0thMUd
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow.keras import layers

df = pd.read_csv('BTC-USD.csv')

df.head()

df.isnull().sum()

df.shape

df.describe()

df['Date'] = pd.to_datetime(df['Date'], format='%Y/%m/%d')

sns.set_style('darkgrid')
plt.figure(figsize=(12,6))
sns.lineplot(data=df, x='Date', y='Close', color='red')
plt.xlabel('Date')
plt.ylabel('Price')
plt.title('Graph of Bitcoin Currency in USD')
plt.show()

sns.set_style('darkgrid')
plt.figure(figsize=(12,6))
sns.lineplot(data=df, x='Date', y='Volume', color='red')
plt.xlabel('Date')
plt.ylabel('Volume')
plt.title('Graph of The Volume Bitcoin Currency Traded')
plt.show()

df.columns

df = df.drop(['Open', 'High', 'Low', 'Adj Close', 'Volume'], axis=1)

df = df.set_index('Date')

df.head()

# Define the state space and action space
state_space = 1
action_space = 3  # Buy, sell, hold

# Define the DQN model
def build_model(state_space, action_space):
    model = tf.keras.Sequential([
        layers.Dense(64, activation='relu', input_shape=(state_space,)),
        layers.Dense(32, activation='relu'),
        layers.Dense(action_space, activation=None)
    ])
    return model

class DQNAgent:
    def __init__(self, state_space, action_space):
        # Initialize the state and action spaces
        self.state_space = state_space
        self.action_space = action_space
        self.epsilon = 1.0
        self.epsilon_decay = 0.995
        self.epsilon_min = 0.01 # Minimum exploration rate
        self.learning_rate = 0.0008
        self.gamma = 0.95  # Discount factor for future rewards
        self.memory = [] # Memory buffer to store experiences
        self.batch_size = 16
        self.model = build_model(state_space, action_space) # Build the deep Q-network model

    def remember(self, state, action, reward, next_state, done):
        # Store the experience in memory buffer
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        # Choose an action based on epsilon-greedy policy
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.action_space)
        # Use the Q-network to predict Q-values for each action
        q_values = self.model.predict(state)
        return np.argmax(q_values[0])

    def replay(self):
         # Sample a mini-batch of experiences from memory buffer
        if len(self.memory) < self.batch_size:
            return
        indices = np.random.choice(len(self.memory), size=self.batch_size, replace=False)
        batch = [(self.memory[i][0], self.memory[i][1], self.memory[i][2], self.memory[i][3], self.memory[i][4]) for i in indices]
        # Update the Q-values for the selected experiences
        for state, action, reward, next_state, done in batch:
            q_update = reward
            if not done:
                # Use the Q-network to predict Q-values for the next state
                q_update = reward + self.gamma * np.amax(self.model.predict(next_state)[0])
            # Use the Q-network to predict Q-values for the current state
            q_values = self.model.predict(state)
            q_values[0][action] = q_update
            # Train the Q-network on the updated Q-values
            self.model.fit(state, q_values, verbose=0)
             # Decay the exploration rate
        self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)

# Create the DQN agent
Bitcoin_agent = DQNAgent(state_space, action_space)

# Train the DQN agent
max_iterations = 10
iteration = 0
for i in range(5):
    state = df.iloc[0]
    print("Shape of state:", state.shape)
    total_profit = 0
    for t in range(len(df)-1):
        action = Bitcoin_agent.act(state)
        next_state = df.iloc[t+1]
        reward = 0
        if action == 0:  # Buy
            profit = df.iloc[t+1]['Close'] - df.iloc[t]['Close']
            if profit > 0:
                reward = 1
            else:
                reward = -1
            total_profit += profit
        elif action == 1:  # Sell
            profit = df.iloc[t]['Close'] - df.iloc[t+1]['Close']
            if profit > 0:
                reward = 1
            else:
                reward = -1
            total_profit += profit
        else:  # Hold
            profit = 0
        Bitcoin_agent.remember(state, action, reward, next_state, False)
        state = next_state

        # Define the experience tuple
        e = (state, action, reward, next_state, False)

        # Train the agent on the updated experiences
        Bitcoin_agent.model.compile(optimizer='adam', loss='mse')
        Bitcoin_agent.model.fit(x=np.array([e[0] for e in Bitcoin_agent.memory]),
                                y=np.array([np.array([r + Bitcoin_agent.gamma * np.max(Bitcoin_agent.model.predict(e[3])[0])
                                            if not e[4] else r]) for r in np.array([e[2] for e in Bitcoin_agent.memory])]),
                                epochs=10, verbose=2)
        # Decay the exploration rate
        Bitcoin_agent.epsilon = max(Bitcoin_agent.epsilon * Bitcoin_agent.epsilon_decay, Bitcoin_agent.epsilon_min)

        # Increment the iteration counter
        iteration += 1

        # Check if max_iterations has been reached
        if iteration >= max_iterations:
            break

    print('Episode: {}, Total Profit: {}'.format(i, total_profit))
print("Training has finished now you can save the model.")

Bitcoin_agent.model.save('Bitcoin_DQN_model.h5')

df.shape

from keras.models import load_model

# Load the saved model
Bitcoin_DQN_model = load_model('Bitcoin_DQN_model.h5')

# Use the model to predict the next 30 days' bitcoin prices to guide trading decisions.
next_30_days = pd.concat([df.iloc[[-1]]] * 30).reset_index(drop=True)
next_30_days *= (1 + np.random.normal(0, 0.1, (30, df.shape[1])))

for t in range(30):
    state = next_30_days.iloc[t]
    action = Bitcoin_agent.act(state)
    if action == 0:
        print("Day {}: Predicted action: Buy".format(t+1))
    elif action == 1:
        print("Day {}: Predicted action: Sell".format(t+1))
    else:
        print("Day {}: Predicted action: Hold".format(t+1))

# Load the saved model
Bitcoin_DQN_model = load_model('Bitcoin_DQN_model.h5')

# Convert to weekly frequency
weekly_df = df.resample('W').last()

# Use the model to predict the next 20 weeks' bitcoin prices to guide trading decisions.
next_20_weeks = pd.concat([weekly_df.iloc[[-1]]] * 20).reset_index(drop=True)
next_20_weeks *= (1 + np.random.normal(0, 0.1, (20, weekly_df.shape[1])))

for t in range(20):
    state = next_20_weeks.iloc[t]
    action = Bitcoin_agent.act(state)
    if action == 0:
        print("Week {}: Predicted action: Buy".format(t+1))
    elif action == 1:
        print("Week {}: Predicted action: Sell".format(t+1))
    else:
        print("Week {}: Predicted action: Hold".format(t+1))

# Load the saved model
Bitcoin_DQN_model = load_model('Bitcoin_DQN_model.h5')

# Convert to weekly frequency
monthly_df = df.resample('M').last()

# Use the model to predict the next 6 months' bitcoin prices to guide trading decisions.
next_6_months = pd.concat([monthly_df.iloc[[-1]]] * 6).reset_index(drop=True)
next_6_months *= (1 + np.random.normal(0, 0.1, (6, monthly_df.shape[1])))

for t in range(6):
    state = next_6_months.iloc[t]
    action = Bitcoin_agent.act(state)
    if action == 0:
        print("Month {}: Predicted action: Buy".format(t+1))
    elif action == 1:
        print("Month {}: Predicted action: Sell".format(t+1))
    else:
        print("Month {}: Predicted action: Hold".format(t+1))