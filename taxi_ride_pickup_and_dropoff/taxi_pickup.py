# -*- coding: utf-8 -*-
"""taxi_pickup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m2Fyi3X4UnwNONEwCxPNvp-_J3EKgtpI

RL on a taxi
"""

!apt install swig cmake

!pip install setuptools==65.5.0
!pip install stable-baselines3[extra]
!pip install box2d-py
!pip install huggingface_sb3
!pip install pyglet
!pip install gym[box2d]
!sudo apt-get update
!pip install gym
!apt install python-opengl
!apt install ffmpeg
!apt install xvfb
!pip3 install pyvirtualdisplay

import gym
from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
from huggingface_hub import notebook_login
from collections import deque
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy
import os
from stable_baselines3.common.vec_env import VecFrameStack

# Virtual display
from pyvirtualdisplay import Display

virtual_display = Display(visible=0, size=(1400, 900))
virtual_display.start()

env = gym.make("Taxi-v3")

!pip uninstall -y pyglet
!pip install pyglet==1.5.0

# Create environment
env = gym.make("Taxi-v3")

observation = env.reset()

for _ in range(10):
    #take random action
    action = env.action_space.sample()
    print("Action taken:", action)
    # Render the environment
    env.render()

    observation, reward, done, info = env.step(action)

    #if the game is done
    if done:
        #Reset the environment
        observation = env.reset()
        print("Environment reset")

env = make_vec_env("Taxi-v3", n_envs = 10)

from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import VecNormalize

import os

# Create a directory for logs
if not os.path.exists("./logs"):
    os.mkdir("./logs")

# Set the log path
log_path = "./logs/PPO_1"

# Create environment
env = gym.make("Taxi-v3")


model = PPO(
    policy="MlpPolicy",
    env=env,
    learning_rate=0.00005,  # lower learning rate for stability
    n_steps=2048,  # increase n_steps for more efficient training
    batch_size=64,  # increase batch size for more efficient training
    n_epochs=1000,
    gamma=0.99,  # adjust gamma to balance immediate vs future rewards
    gae_lambda=0.95,  # adjust lambda to balance bias vs variance in advantage estimation
    clip_range=0.25,  # adjust clip range for policy gradient optimization
    clip_range_vf=None,  # disable clipping for value function optimization
    ent_coef=0.0,  # adjust entropy coefficient to balance exploration vs exploitation
    vf_coef=0.7,  # adjust value function coefficient to balance policy gradient and value function optimization
    max_grad_norm=0.6,  # setting gradient clipping
    tensorboard_log= log_path,
    verbose=1
)

print(model)

import time

# Train the agent for 10000 timesteps
start = time.time()
model.learn(total_timesteps=10000)
end = time.time()

# Print the total training time
print(f"Total training time: {end - start:.2f} seconds")

# save the model 
model_name = "taxi_model"
model.save(model_name)

# Create a new environment for evaluation
eval_env = gym.make('Taxi-v3')

# Evaluate the model 
mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True , render= True)

# Print the results

print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")